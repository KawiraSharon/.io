{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr+5Qnm2dVnINtDauPiHVe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KawiraSharon/.io/blob/main/Project_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22NRe1GvZ_G"
      },
      "outputs": [],
      "source": [
        "#Extraction transformation and loading yaml file\n",
        "id: nycTaxi\n",
        "namespace: kestra-task\n",
        "\n",
        "tasks:\n",
        "  - id: extracting_nyc_taxi\n",
        "    type: io.kestra.plugin.scripts.python.Script\n",
        "    taskRunner:\n",
        "      type: io.kestra.plugin.core.runner.Process\n",
        "    beforeCommands:\n",
        "      - python3 -m venv .venv\n",
        "      - . .venv/bin/activate\n",
        "      - pip install pandas numpy requests pyarrow\n",
        "    outputFiles:\n",
        "      - nyc_taxi_data.csv\n",
        "    warningOnStdErr: false\n",
        "    script: |\n",
        "      import pandas as pd\n",
        "      import numpy as np\n",
        "      import requests\n",
        "      import pyarrow\n",
        "      import re\n",
        "      from io import BytesIO\n",
        "\n",
        "      #Data extraction\n",
        "      #Setting Kaggle API credentials\n",
        "      KAGGLE_USERNAME = \"sharonmungania\"\n",
        "      KAGGLE_KEY = \"c4942b83fcf67954e6e6a6709feecdea\"\n",
        "\n",
        "      #Downloading the zip file directly\n",
        "      url = \"https://www.kaggle.com/api/v1/datasets/download/albertjavier/yellow-tripdata-2023\"\n",
        "      headers = {\"Authorization\": f\"Bearer {KAGGLE_KEY}\"}\n",
        "      response = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "      #Extracting the CSV from the ZIP file since Kaggle downloads datasets as ZIP\n",
        "      from zipfile import ZipFile\n",
        "      with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "        with zip_file.open(\"yellow_tripdata_2023-01.parquet\") as parquet_file:\n",
        "            df = pd.read_parquet(parquet_file)\n",
        "      #Saving data to CSV file\n",
        "      df.to_csv('nyc_taxi_data.csv', index=False)\n",
        "  - id: data_transformation\n",
        "    type: io.kestra.plugin.scripts.python.Script\n",
        "    outputFiles:\n",
        "      - transformed_data.csv\n",
        "    taskRunner:\n",
        "      type: io.kestra.plugin.core.runner.Process\n",
        "    beforeCommands:\n",
        "      - python3 -m venv .venv\n",
        "      - . .venv/bin/activate\n",
        "      - pip install pandas\n",
        "    warningOnStdErr: false\n",
        "    script: |\n",
        "      import pandas as pd\n",
        "      #Loading car sases data from the CSV file\n",
        "      df = pd.read_csv('{{ outputs.extracting_nyc_taxi.outputFiles[\"nyc_taxi_data.csv\"] }}')\n",
        "      # removing ghost trips and trips without passengers\n",
        "      new_df = df[(df['passenger_count'] > 0) & (df['trip_distance'] > 0)].copy()\n",
        "      # normalize datetime columns\n",
        "      new_df['tpep_pickup_datetime'] = pd.to_datetime(new_df['tpep_pickup_datetime'])\n",
        "      new_df['tpep_dropoff_datetime'] = pd.to_datetime(new_df['tpep_dropoff_datetime'])\n",
        "      # creating column trip_duration for visualization\n",
        "      new_df['trip_duration'] = (new_df['tpep_dropoff_datetime'] - new_df  ['tpep_pickup_datetime']).dt.total_seconds()/60\n",
        "      # removing trips longer than 180 mins/3hrs\n",
        "      new_df = new_df[new_df['trip_duration'] <= 180]\n",
        "      new_df = new_df.sample(n=20000, random_state=42)\n",
        "      new_df.to_csv('transformed_data.csv', index=False)\n",
        "  - id: loading_nyc_taxi_data\n",
        "    type: io.kestra.plugin.scripts.python.Script\n",
        "    taskRunner:\n",
        "      type: io.kestra.plugin.core.runner.Process\n",
        "    beforeCommands:\n",
        "      - python3 -m venv .venv\n",
        "      - . .venv/bin/activate\n",
        "      - pip install pandas psycopg2-binary\n",
        "\n",
        "    warningOnStdErr: false\n",
        "    script: |\n",
        "      import pandas as pd\n",
        "      import psycopg2\n",
        "      from psycopg2.extras import execute_batch\n",
        "\n",
        "      # Load the transformed data from the stored CSV\n",
        "      df = pd.read_csv('{{ outputs.data_transformation.outputFiles[\"transformed_data.csv\"] }}')\n",
        "\n",
        "      # Connect to the PostgreSQL database\n",
        "      conn = psycopg2.connect(\n",
        "          host=\"postgres\",\n",
        "          database=\"kestra\",\n",
        "          user=\"kestra\",\n",
        "          password=\"k3str4\"\n",
        "      )\n",
        "      cursor = conn.cursor()\n",
        "\n",
        "      # Create the table if it doesn't exist\n",
        "      create_table_query = \"\"\"\n",
        "      CREATE TABLE IF NOT EXISTS nyc_taxi_trips (\n",
        "          VendorID INT,\n",
        "          tpep_pickup_datetime TIMESTAMP WITH TIME ZONE,\n",
        "          tpep_dropoff_datetime TIMESTAMP WITH TIME ZONE,\n",
        "          passenger_count INT,\n",
        "          trip_distance FLOAT,\n",
        "          RatecodeID INT,\n",
        "          store_and_fwd_flag VARCHAR,\n",
        "          PULocationID INT,\n",
        "          DOLocationID INT,\n",
        "          payment_type INT,\n",
        "          fare_amount FLOAT,\n",
        "          extra FLOAT,\n",
        "          mta_tax FLOAT,\n",
        "          tip_amount FLOAT,\n",
        "          tolls_amount FLOAT,\n",
        "          improvement_surcharge FLOAT,\n",
        "          total_amount FLOAT,\n",
        "          congestion_surcharge FLOAT,\n",
        "          airport_fee FLOAT,\n",
        "          trip_duration INT\n",
        "      )\n",
        "      \"\"\"\n",
        "      cursor.execute(create_table_query)\n",
        "      conn.commit()\n",
        "\n",
        "      # Prepare data for bulk insert\n",
        "      data_to_insert = [\n",
        "          (\n",
        "              row['VendorID'], row['tpep_pickup_datetime'], row['tpep_dropoff_datetime'],\n",
        "              row['passenger_count'], row['trip_distance'], row['RatecodeID'],\n",
        "              row['store_and_fwd_flag'], row['PULocationID'], row['DOLocationID'],\n",
        "              row['payment_type'], row['fare_amount'], row['extra'], row['mta_tax'],\n",
        "              row['tip_amount'], row['tolls_amount'], row['improvement_surcharge'],\n",
        "              row['total_amount'], row['congestion_surcharge'], row['airport_fee'],\n",
        "              row['trip_duration']\n",
        "          )\n",
        "          for index, row in df.iterrows()\n",
        "      ]\n",
        "\n",
        "      insert_query = \"\"\"\n",
        "      INSERT INTO nyc_taxi_trips (\n",
        "          VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance,\n",
        "          RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount,\n",
        "          extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount,\n",
        "          congestion_surcharge, airport_fee, trip_duration\n",
        "      ) VALUES (\n",
        "          %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
        "      )\n",
        "      \"\"\"\n",
        "      execute_batch(cursor, insert_query, data_to_insert)\n",
        "      conn.commit()\n",
        "\n"
      ]
    }
  ]
}